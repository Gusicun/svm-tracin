{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from linear_svm import *\n",
    "from softmax import *\n",
    "from sklearn import datasets,cross_validation\n",
    "import torch\n",
    "#from torch.autograd import grad\n",
    "\n",
    "class LinearClassifier(object):\n",
    "\n",
    "  def __init__(self):\n",
    "    self.W = None\n",
    "\n",
    "  def tracin(self,grad_z_train,batch_size,W,iters,reg):\n",
    "    data=datasets.load_iris()\n",
    "    X_train = data.data\n",
    "    y_train = data.target\n",
    "    X_train, X_test, y_train, y_test = cross_validation.train_test_split(X_train, y_train, test_size=0.25,\n",
    "                                                                         random_state=0, stratify=y_train)\n",
    "    num_training = 100\n",
    "    num_validation = 12\n",
    "    num_test = 38\n",
    "    num_dev = 50\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_tracin = X_train[mask]\n",
    "    y_tracin = y_train[mask]\n",
    "    num_tracin, dim = X_tracin.shape\n",
    "    num_classes = np.max(y_tracin) + 1\n",
    "\n",
    "    if self.W is None:\n",
    "      # lazily initialize W\n",
    "      self.W = 0.001 * np.random.randn(dim, num_classes)\n",
    "\n",
    "    # Run stochastic gradient descent to optimize W\n",
    "    loss_history = []\n",
    "    for it in range(num_tracin):\n",
    "      X_batch = None\n",
    "      y_batch = None\n",
    "\n",
    "      batch_idx = np.random.choice(num_tracin, batch_size, replace=True)  # 这里是随机选一个，考虑锚点的话应该改成顺序选择\n",
    "      print(\"this point is Point_{}\".format(batch_idx))\n",
    "      X_batch = X_tracin[batch_idx]\n",
    "      y_batch = y_tracin[batch_idx]\n",
    "\n",
    "      # evaluate loss and gradient\n",
    "      loss, grad = self.loss(X_batch, y_batch, reg)\n",
    "      grad_z_test = grad#self.get_grad(grad, batch_idx)\n",
    "      score=tracin_get(grad_z_train,grad_z_test)\n",
    "      ###这里插入tracin，但是要考虑到循环问题\n",
    "      loss_history.append(loss)\n",
    "  def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100,\n",
    "            batch_size=1, verbose=False):\n",
    "    \"\"\"\n",
    "    Train this linear classifier using stochastic gradient descent.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) containing training data; there are N\n",
    "      training samples each of dimension D.\n",
    "    - y: A numpy array of shape (N,) containing training labels; y[i] = c\n",
    "      means that X[i] has label 0 <= c < C for C classes.\n",
    "    - learning_rate: (float) learning rate for optimization.\n",
    "    - reg: (float) regularization strength.\n",
    "    - num_iters: (integer) number of steps to take when optimizing\n",
    "    - batch_size: (integer) number of training examples to use at each step.\n",
    "    - verbose: (boolean) If true, print progress during optimization.\n",
    "\n",
    "    Outputs:\n",
    "    A list containing the value of the loss function at each training iteration.\n",
    "    \"\"\"\n",
    "    num_train, dim = X.shape\n",
    "    num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n",
    "    if self.W is None:\n",
    "      # lazily initialize W\n",
    "      self.W = 0.001 * np.random.randn(dim, num_classes)\n",
    "\n",
    "    # Run stochastic gradient descent to optimize W\n",
    "    loss_history = []\n",
    "    for it in range(num_iters):\n",
    "      X_batch = None\n",
    "      y_batch = None\n",
    "\n",
    "      batch_idx = np.random.choice(num_train, batch_size, replace = True)#这里是随机选一个，考虑锚点的话应该改成顺序选择\n",
    "      print(\"this point is Point_{}\".format(batch_idx))\n",
    "      X_batch =  X[batch_idx]\n",
    "      y_batch = y[batch_idx]\n",
    "\n",
    "      # evaluate loss and gradient\n",
    "      loss, grad = self.loss(X_batch, y_batch, reg)\n",
    "      #grad_z_train=grad#self.get_grad(grad,batch_idx)\n",
    "      #tracin_score=self.tracin(grad_z_train,batch_size,100,reg)\n",
    "      #print(\"Tracin score for SVM on sample {} is {}\".format(batch_idx,tracin_score))\n",
    "      ###这里插入tracin，但是要考虑到循环问题\n",
    "      loss_history.append(loss)\n",
    "\n",
    "      # perform parameter update\n",
    "\n",
    "      self.W += - learning_rate * grad\n",
    "\n",
    "\n",
    "      if verbose and it % 100 == 0:\n",
    "        print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
    "\n",
    "    return loss_history\n",
    "\n",
    "  def predict(self, X):\n",
    "    \"\"\"\n",
    "    Use the trained weights of this linear classifier to predict labels for\n",
    "    data points.\n",
    "\n",
    "    Inputs:\n",
    "    - X: D x N array of training data. Each column is a D-dimensional point.\n",
    "\n",
    "    Returns:\n",
    "    - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
    "      array of length N, and each element is an integer giving the predicted\n",
    "      class.\n",
    "    \"\"\"\n",
    "    y_pred = np.zeros(X.shape[1])\n",
    "    ###########################################################################\n",
    "    # TODO:                                                                   #\n",
    "    # Implement this method. Store the predicted labels in y_pred.            #\n",
    "    ###########################################################################\n",
    "    scores = X.dot(self.W)\n",
    "    y_pred = np.argmax(scores, axis = 1)\n",
    "    #pass\n",
    "    ###########################################################################\n",
    "    #                           END OF YOUR CODE                              #\n",
    "    ###########################################################################\n",
    "    return y_pred\n",
    "  \n",
    "  def loss(self, X_batch, y_batch, reg):\n",
    "    \"\"\"\n",
    "    Compute the loss function and its derivative. \n",
    "    Subclasses will override this.\n",
    "\n",
    "    Inputs:\n",
    "    - X_batch: A numpy array of shape (N, D) containing a minibatch of N\n",
    "      data points; each point has dimension D.\n",
    "    - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n",
    "    - reg: (float) regularization strength.\n",
    "\n",
    "    Returns: A tuple containing:\n",
    "    - loss as a single float\n",
    "    - gradient with respect to self.W; an array of the same shape as W\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class LinearSVM(LinearClassifier):\n",
    "  \"\"\" A subclass that uses the Multiclass SVM loss function \"\"\"\n",
    "\n",
    "  def loss(self, X_batch, y_batch, reg):\n",
    "    return svm_loss_vectorized(self.W, X_batch, y_batch, reg)\n",
    "\n",
    "\n",
    "class Softmax(LinearClassifier):\n",
    "  \"\"\" A subclass that uses the Softmax + Cross-entropy loss function \"\"\"\n",
    "\n",
    "  def loss(self, X_batch, y_batch, reg):\n",
    "    return softmax_loss_vectorized(self.W, X_batch, y_batch, reg)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
